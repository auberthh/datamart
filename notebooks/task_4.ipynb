{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4: Modelado Predictivo \n",
    "\n",
    "### By:\n",
    "Auberth Eduardo Hurtado\n",
    "\n",
    "### Date:\n",
    "2025-01-17\n",
    "\n",
    "### Description:\n",
    "\n",
    "Create a predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Load the cleaned_sales_data.csv file from the datamart/data/final directory and inspect the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned sales data\n",
    "sales_data = pd.read_csv(\"../data/final/cleaned_sales_data.csv\")\n",
    "sales_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 1. Realiza un análisis exploratorio avanzado: \n",
    "- Identifica estacionalidad y tendencias. \n",
    "- Detecta outliers y evalúa su impacto en las ventas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying seasonality and trends\n",
    "sales_data['date'] = pd.to_datetime(sales_data['order_date'])\n",
    "data = sales_data.set_index('date')\n",
    "\n",
    "monthly_revenue = data['total_revenue'].resample('ME').sum()\n",
    "monthly_revenue.plot(figsize=(14, 7), title='Monthly Revenue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis by month\n",
    "monthly_decomposition = seasonal_decompose(monthly_revenue, model='additive')\n",
    "# Plot the decomposed components\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(monthly_decomposition.observed, label='Observed')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(412)\n",
    "plt.plot(monthly_decomposition.trend, label='Trend')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(413)\n",
    "plt.plot(monthly_decomposition.seasonal, label='Seasonal')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(414)\n",
    "plt.plot(monthly_decomposition.resid, label='Residual')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result suggests the existence of a stochastic function. In other words, total revenue can be estimated as $Revenue = trend + seasonality - 3.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis by day\n",
    "daily_revenue = data['total_revenue'].resample('D').sum()\n",
    "daily_decomposition = seasonal_decompose(daily_revenue, model='additive', period=365)\n",
    "\n",
    "# Plot the decomposed components\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(daily_decomposition.observed, label='Observed')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(412)\n",
    "plt.plot(daily_decomposition.trend, label='Trend')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(413)\n",
    "plt.plot(daily_decomposition.seasonal, label='Seasonal')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(414)\n",
    "plt.plot(daily_decomposition.resid, label='Residual')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result suggests the existence of no erros. In other words, daily total revenue can be estimated as $trend + seasonality$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection of outliers and evaluation of their impact on sales\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.boxplot(data=data, x='region', y='total_revenue')\n",
    "plt.title('Boxplot of Total Revenue by Region')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers have a positive impact by increasing the total revenue in the final estimation. If we exclude the outliers, the average revenue will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define una estrategia para predecir el ingreso diario total: \n",
    "- Divide los datos en entrenamiento (80%) y prueba (20%). \n",
    "- Implementa modelos: \n",
    "    - Regresión lineal múltiple considerando quantity, price, discount, y region.  \n",
    "    - Modelo basado en Random Forest o Gradient Boosting para capturar relaciones no lineales. \n",
    "- Opcional: Usa técnicas de feature engineering, como generación de variables temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "data['day_of_week'] = data.index.dayofweek\n",
    "data['month'] = data.index.month\n",
    "data['day_of_month'] = data.index.day\n",
    "data['week_of_year'] = data.index.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to dummy variables\n",
    "data = pd.get_dummies(data, columns=['region', 'product_id'], drop_first=True)\n",
    "# for testing purposes other option is use one hot encoder of scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data in train and test sets\n",
    "X = data[['quantity', 'price', 'discount', 'day_of_week', 'month', 'day_of_month', 'week_of_year'] + [col for col in data.columns if 'region_' in col or 'product_id_' in col]]\n",
    "y = data['total_revenue']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.head()\n",
    "#X_test.head()\n",
    "#y_train.head()\n",
    "#y_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models implementation\n",
    "\n",
    "# A. Linear regression\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "\n",
    "# B. Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# C. Gradient Boosting\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of the test, I have run the models with their default parameters; however, some adjustments could be made to improve the initial comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evalúa los modelos usando: \n",
    "- MAE, R^2, y análisis de residuales. \n",
    "- Describe el rendimiento y si el modelo es aplicable en un entorno real. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar function to evaluate the models\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f'{model_name} - MAE: {mae:.2f}, R^2: {r2:.2f}')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.residplot(x=y_true, y=y_pred, lowess=True, line_kws={'color': 'red', 'lw': 1})\n",
    "    plt.title(f'Residuals Plot for {model_name}')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar Regresión Lineal Múltiple\n",
    "evaluate_model(y_test, y_pred_linear, 'Linear Regression')\n",
    "\n",
    "# Evaluar Random Forest\n",
    "evaluate_model(y_test, y_pred_rf, 'Random Forest')\n",
    "\n",
    "# Evaluar Gradient Boosting\n",
    "evaluate_model(y_test, y_pred_gb, 'Gradient Boosting')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💡 Descripción del rendimiento y aplicabilidad en un entorno real\n",
    "Descripción del rendimiento y aplicabilidad en un entorno real:\n",
    "- **La regresión lineal múltiple** puede ser útil para relaciones lineales simples, pero puede no capturar todas las complejidades, en este caso se observa que el modelo falla en estimar los ingresos con valores más pequeños presentando un problema de heterocedasticidad.\n",
    "- **Random Forest** y **Gradient Boosting** son más efectivos para capturar relaciones no lineales y pueden ofrecer mejores predicciones, entre los dos le fue mejor a Random forest al encontrar bajor error en sus estimaciones.\n",
    "- Evaluar el MAE y $R^2$ ayuda a entender la precisión y la capacidad explicativa de los modelos, aunque considero que $R^2$ debería ser revisado como medida de bondad de ajuste al final de todo el proceso pues es muy sensible a los supuestos del modelo, idealmente acompañarlo con otras medidas como RMSE o MAPE.\n",
    "- En un entorno real, se debe considerar la interpretabilidad del modelo, el tiempo de entrenamiento y la capacidad de generalización. El modelo elegible podría ser random forest pero faltaría profundizar más y examinar otras alternativas como el ajuste de hiperparámetros u otros modelos más idones para este tipo de problemas.\n",
    "\n",
    "----\n",
    "\n",
    "- **Multiple Linear Regression** can be useful for simple linear relationships but may not capture all complexities. In this case, the model struggles to estimate revenues with smaller values, presenting a problem of heteroscedasticity.\n",
    "\n",
    "- **Random Forest** and **Gradient Boosting** are more effective in capturing non-linear relationships and can provide better predictions. Among the two, Random Forest performed better, exhibiting lower error in its estimations.\n",
    "\n",
    "- Evaluating MAE and $R^2$ helps in understanding the accuracy and explanatory power of the models. However $R^2$ should be reviewed as a goodness-of-fit measure at the end of the entire process, as it is very sensitive to the model's assumptions. Ideally, it should be complemented with other measures such as RMSE or MAPE.\n",
    "\n",
    "- In a real-world environment, model interpretability, training time, and generalization capability should be considered. While Random Forest could be a viable model, further exploration and examination of other alternatives, such as hyperparameter tuning or other more suitable models for this type of problem, are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 Be cautious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final and processed dataset\n",
    "data.to_csv(r'../data/final/final_sales_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "joblib.dump(rf_model, r'../models/random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamart_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
